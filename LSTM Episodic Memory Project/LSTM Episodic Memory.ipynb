{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Motivation\n",
    "\n",
    "Reinforcement agents are notoriously difficult to train due the commonly encountered problem of reduced learning rates on data previously used to train the agent. Many approaches are currently being made to remedy the problem, one of which is episodic recall in the fashion of [Ritter et al. (2018)](https://arxiv.org/pdf/1805.09692.pdf). This method involves a stable external memory usually implemented as a differentiable neural dictionary (DND) of key value pairs which link previously encountered environment states with the network's cell states at the time the environment states were encountered. This essentially allows temporally stable recall of contexts that can be useful when environment states which are similar but not identical to those found within its DND memory are encountered. Long short-term memory neural network (LSTM) architecture is then augmented with an additional input gate from the DND and an output gate to store the current context within it.\n",
    "\n",
    "While episodic recall itself is a promising route, there are still technical details involved in ensuring it actually decreases the amount of data needed for training and training time. One such issue is the way key-value pairs are embedded and recalled from the DND. Here I implement a DND based on the description in [Pritzel et al. (2017)](https://arxiv.org/pdf/1703.01988.pdf) including their kernel function. To recall cell states from the DND there are various methods of determining the relative similarity of environment states in the memory to that currently encountered. This can be specified by a kernel function which weights the corresponding cell states based on a metric such as\n",
    "$$k(h,h_{i}) = \\frac{1}{||h-h_{i}||_{2}^{2} + \\delta}$$\n",
    "\n",
    "where $h$ is the current environment state, $h_{i}$ are memorised environment states which are being included in the recall, and $\\delta$ is a distance tuning parameter. \n",
    "\n",
    "Below I have an implementation of the modified LSTM architecture and its DND. At the moment I am exploring alternative methods to using p-nearest neighbours variants to extract the cell states from the DND as done in [Pritzel et al. (2017)](https://arxiv.org/pdf/1703.01988.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, Flux, Statistics, Dates, TimeZones, PyCall, Plots\n",
    "ast_stats = pyimport(\"astropy.stats\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Importing and Cleaning\n",
    "\n",
    "<font size=\"3.5\">The data used for the training of the reinforcement learning agent seen below is minute gold futures data from COMEX GCZ2020 between September 29, 2020 at 16:43 to October 6, 2020 at 04:06.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# The functions time_change, find_date, data_ret, and line_prediction are functions needed\n",
    "# and solely use for the importation and cleaning of the financial data I am using.\n",
    "#\n",
    "\n",
    "\n",
    "function time_change(T,mins;date_time=false)\n",
    "    if date_time === false\n",
    "        date_time = DateTime(T,\"dd-u-yyyy HH:MM\") + Minute(mins)\n",
    "        time = Dates.format(date_time, \"dd-u-yyyy HH:MM\")\n",
    "    \n",
    "        return date_time, time\n",
    "    else\n",
    "        date_time = T + Minute(mins)\n",
    "        time = Dates.format(date_time, \"dd-u-yyyy HH:MM\")\n",
    "        \n",
    "        return date_time, time\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function find_date(elements, value; days=false)\n",
    "    \n",
    "    \n",
    "    left = 1\n",
    "    right = length(elements)\n",
    "\n",
    "    first = 1\n",
    "    last = length(elements)\n",
    "        \n",
    "    while left <= right\n",
    "        middle = (left + right) รท 2\n",
    "        middle_element = elements[middle][5]\n",
    "        \n",
    "        if middle != first && middle != last\n",
    "            left_element = elements[middle-1][5]\n",
    "            right_element = elements[middle+1][5]\n",
    "\n",
    "            if middle_element == value\n",
    "                return middle_element,middle\n",
    "            end\n",
    "            if left_element == value\n",
    "                return left_element,middle-1\n",
    "            end\n",
    "            if right_element == value\n",
    "                return right_element,middle+1\n",
    "            end\n",
    "            if middle - left <= 1 && right - middle <= 1\n",
    "                times_under_consideration = Dict()\n",
    "                datetime_triple = [(left_element,left_element,middle - 1),\n",
    "                                   (middle_element,middle_element,middle),\n",
    "                                   (right_element,right_element,middle + 1)]\n",
    "                for c in 1:length(datetime_triple)\n",
    "                    if value > datetime_triple[c][1]\n",
    "                        times_under_consideration[value - datetime_triple[c][1]] = (datetime_triple[c][2],datetime_triple[c][3])\n",
    "                    end\n",
    "                end\n",
    "                if length(times_under_consideration) > 0\n",
    "                    to_be_returned = times_under_consideration[minimum(keys(times_under_consideration))] \n",
    "                    return to_be_returned\n",
    "                end\n",
    "            end\n",
    "        elseif middle == first || middle == last\n",
    "            if value <= middle_element\n",
    "                return middle_element, middle\n",
    "            end\n",
    "            if middle_element <= value\n",
    "                return middle_element, middle\n",
    "            end\n",
    "        end\n",
    "                \n",
    "        if middle_element < value\n",
    "            left = middle + 1\n",
    "        elseif middle_element > value\n",
    "            right = middle - 1\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function data_ret(T_date;data_location=data_array,A=0,price_grab=false)\n",
    "    \n",
    "    Keys = []\n",
    "    Data = []\n",
    "    \n",
    "    @views begin\n",
    "    if price_grab === true\n",
    "        time, index = find_date(data_location[1],T_date)\n",
    "        push!(Keys,(1,index))\n",
    "        push!(Data,data_location[1][index])\n",
    "        return Data[1][9]\n",
    "    end\n",
    "    \n",
    "    for c in 1:length(data_location)\n",
    "        if c !== 9\n",
    "            time, index = find_date(data_location[c],T_date)\n",
    "            push!(Keys,(c,index))\n",
    "            push!(Data,data_location[c][index])\n",
    "        else\n",
    "            time, index = find_date(data_location[c],T_date;days=true)\n",
    "            push!(Keys,(c,index))\n",
    "            push!(Data,data_location[c][index])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    dP_dt = [Data[c][2] for c in 1:length(Data)]\n",
    "    dW_dt = [Data[c][1] for c in 1:length(Data)]\n",
    "    d2P_dt2 = []\n",
    "    d2W_dt2 = []\n",
    "    end\n",
    "        \n",
    "    if A !== 0\n",
    "        A += 1\n",
    "        y_list = [(data_location[1][Keys[1][2]+-c][7]+data_location[1][Keys[1][2]+-c][8])/2 for c in 1:A]\n",
    "        return dP_dt, dW_dt, d2P_dt2, d2W_dt2, Keys, Data, y_list\n",
    "    end\n",
    "\n",
    "    return dP_dt, dW_dt, d2P_dt2, d2W_dt2, Keys, Data\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function line_prediction(t,dP_dt,dW_dt;d2P_dt2=0,d2W_dt2=0,gamma=1,kappa=0,Price=Price)\n",
    "    \n",
    "    predictions_difference = []\n",
    "    for x in eachindex(dP_dt)\n",
    "        push!(predictions_difference,((dP_dt[x]*(0-(t/periods[x]))+((0-(t/periods[x]))*(dW_dt[x]-dP_dt[x])*gamma) + Price)))\n",
    "    end\n",
    "    \n",
    "        \n",
    "    total = []\n",
    "    \n",
    "    for c in eachindex(predictions_difference)\n",
    "        if abs(t-periods[c]) !== 0\n",
    "            push!(total,(predictions_difference[c]/abs(t-periods[c])))\n",
    "        else\n",
    "            push!(total,predictions_difference[c])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    average = mean(total)\n",
    "    \n",
    "    for c in eachindex(total)\n",
    "        total[c] = (total[c]/average)*predictions_difference[c]\n",
    "    end\n",
    "\n",
    "    prediction = mean(total)\n",
    "    \n",
    "    return prediction\n",
    "end\n",
    "\n",
    "\n",
    "periods = [1,2,5,10,30,60,120,240,1440]\n",
    "\n",
    "\n",
    "#\n",
    "# The function load_files loads the data files I am using, cleans the data, and puts it in an appropriate\n",
    "# format for use in training.\n",
    "#\n",
    "\n",
    "function load_files(s,v)\n",
    "\n",
    "    data_list = [[], [], [], [], [], [], [], [], []]\n",
    "\n",
    "    for aa in 1:9\n",
    "        storage_file = s*\"indicator_data_\"*string(aa)*\".csv\"\n",
    "        \n",
    "        io = open(storage_file, \"r\")\n",
    "        lines = readlines(io)\n",
    "        split_lines = [split(lines[x],\",\") for x in 1:length(lines)]\n",
    "\n",
    "        current_data_times = Set([split_lines[x][5] for x in 1:length(split_lines)])\n",
    "\n",
    "        if aa != 9\n",
    "            transform = Dict(1=>x->parse(Float64,x),2=>x->parse(Float64,x),3=>x->parse(Float64,x),4=>x->parse(Int8,x),5=>x->DateTime(x,\"dd-u-yyyy HH:MM\"),6=>x->parse(Float64,x),7=>x->parse(Float64,x),\n",
    "                            8=>x->parse(Float64,x),9=>x->parse(Float64,x),10=>x->x,11=>x->x)\n",
    "        else\n",
    "            transform = Dict(1=>x->parse(Float64,x),2=>x->parse(Float64,x),3=>x->parse(Float64,x),4=>x->parse(Int8,x),5=>x->DateTime(x,\"dd U yyyy\"),6=>x->parse(Float64,x),7=>x->parse(Float64,x),\n",
    "                            8=>x->parse(Float64,x),9=>x->parse(Float64,x),10=>x->x,11=>x->x)\n",
    "        end\n",
    "\n",
    "        @views begin\n",
    "        current_data = [[transform[x](split_lines[c][x]) for x in 1:11] for c in 1:length(split_lines)]\n",
    "        current_data = unique(current_data)\n",
    "        end\n",
    "        close(io)\n",
    "        #end\n",
    "        println(length(current_data))\n",
    "\n",
    "        storage_file = s*\"indicator_storage_aggregate_\"*string(aa)*\".csv\"\n",
    "        io = open(storage_file, \"r\")\n",
    "        lines = readlines(io)\n",
    "        split_lines = [split(lines[x],\",\") for x in 1:length(lines)]\n",
    "\n",
    "\n",
    "        @views begin\n",
    "        aggregate_data = [[transform[x](split_lines[c][x]) for x in 1:11] for c in 1:length(split_lines) if split_lines[c][5] โ current_data_times]\n",
    "        end\n",
    "        append!(current_data,aggregate_data)\n",
    "        close(io)\n",
    "\n",
    "        println(length(current_data))\n",
    "\n",
    "        data_list[aa] = current_data\n",
    "    end\n",
    "\n",
    "    \n",
    "    volume_file = v\n",
    "\n",
    "    io = open(volume_file, \"r\")\n",
    "    lines = readlines(io)\n",
    "    split_lines = [split(lines[x],\",\") for x in 2:length(lines)]\n",
    "    times = [split_lines[x][1] for x in 1:length(split_lines)]\n",
    "    volumes = []\n",
    "    \n",
    "    try\n",
    "        volumes = [parse(Int, split_lines[x][6]) for x in 1:length(split_lines)]\n",
    "    catch ArgumentError\n",
    "        volumes = [convert(Int,parse(Float64, split_lines[x][6])) for x in 1:length(split_lines)]\n",
    "    end\n",
    "    close(io)\n",
    "\n",
    "    try\n",
    "        DateTime(times[1],\"dd-u-yyyy HH:MM\")\n",
    "    catch\n",
    "        times = [Dates.format(astimezone(ZonedDateTime(DateTime(replace(times[x],\"Z\"=>\"\"), \"y-m-dTH:M:S\"), tz\"UTC\"), tz\"America/Toronto\"),\"dd-u-yyyy HH:MM\") for x in 1:length(times)]\n",
    "    end\n",
    "\n",
    "    times = reverse(times)\n",
    "    volumes = reverse(volumes)\n",
    "    times_as_datetime = [DateTime(times[x],\"dd-u-yyyy HH:MM\") for x in 1:length(times)]\n",
    "\n",
    "    data_list[1] = [data_list[1][c] for c in 1:length(data_list[1]) if data_list[1][c][5] โ times_as_datetime]\n",
    "\n",
    "    @views begin\n",
    "    for c in 1:length(data_list)\n",
    "        if c != 9 && length(data_list[c]) > 1\n",
    "            data_list[c] = sort(data_list[c],by= x -> x[5])\n",
    "        elseif length(data_list[c]) > 1\n",
    "            data_list[c] = sort(data_list[c],by= x -> x[5])\n",
    "        end\n",
    "    end\n",
    "    end\n",
    "\n",
    "\n",
    "\n",
    "    data_datetimes = [data_list[1][c][5] for c in 1:length(data_list[1])]\n",
    "\n",
    "    vols = Dict()\n",
    "    data_strings = [Dates.format(data_list[1][c][5], \"dd-u-yyyy HH:MM\") for c in 1:length(data_list[1])]\n",
    "    data_set = Set(data_strings)\n",
    "\n",
    "    @views begin\n",
    "    for c in 1:length(volumes)\n",
    "        if times[c] in data_set\n",
    "            vols[times[c]] = volumes[c]\n",
    "        end\n",
    "    end\n",
    "    end\n",
    "\n",
    "\n",
    "    println(\"data_set\",length(data_set))\n",
    "\n",
    "    println(data_list[1][1][5],data_list[1][length(data_list[1])][5])\n",
    "\n",
    "    vols_tuple = [(times[c],volumes[c]) for c in 1:length(volumes) if times[c] โ data_set]\n",
    "    println(\"vols: \",length(vols))\n",
    "\n",
    "    times_as_datetime = nothing\n",
    "    data_set = nothing\n",
    "    times = nothing\n",
    "    volumes = nothing\n",
    "\n",
    "    looking = 50\n",
    "    d_d = 25\n",
    "\n",
    "    All_Prices = Dict()\n",
    "    All_Predictions = Dict()\n",
    "\n",
    "    pred_deviation = Dict()\n",
    "    Price_deviation = Dict()\n",
    "    difference_deviation = Dict()\n",
    "\n",
    "    gamma = -15\n",
    "\n",
    "    println(\"vols: \",length(vols),\"data_list[1]\",length(data_list[1]))\n",
    "\n",
    "    ordered_vols_values = similar(data_list[1],Int64)\n",
    "    ordered_vols_keys = similar(data_list[1],String)\n",
    "\n",
    "    for i in 1:length(data_list[1])\n",
    "        ordered_vols_keys[i] = vols_tuple[i][1]\n",
    "        ordered_vols_values[i] = vols_tuple[i][2]\n",
    "    end\n",
    "\n",
    "    ordered_price_values = similar(data_list[1],Float64)\n",
    "    ordered_price_keys = similar(data_list[1],String)\n",
    "    ordered_price_keys_datetime = similar(data_list[1],DateTime)\n",
    "\n",
    "    ordered_prediction_values = similar(data_list[1],Float64)\n",
    "    ordered_prediction_keys = similar(data_list[1],String)\n",
    "    ordered_prediction_keys_datetime = similar(data_list[1],DateTime)\n",
    "\n",
    "    for i in 1:length(data_list[1])\n",
    "       @views begin\n",
    "        T = data_strings[i]\n",
    "        T_datetime = data_datetimes[i]\n",
    "        data_T = data_ret(T_datetime;data_location=data_list)\n",
    "        All_Predictions_time_change = time_change(T_datetime,d_d,date_time=true)[2]\n",
    "\n",
    "        All_Prices[T] = data_T[6][1][9]\n",
    "        dW_dt_d = data_T[1]\n",
    "        dP_dt_d = data_T[2]\n",
    "\n",
    "        All_Predictions[All_Predictions_time_change] = line_prediction(d_d,dP_dt_d,dW_dt_d;Price=All_Prices[T],gamma=gamma)\n",
    "\n",
    "        ordered_price_values[i] = data_T[6][1][9]\n",
    "        ordered_price_keys[i] = T\n",
    "        ordered_price_keys_datetime[i] = T_datetime\n",
    "\n",
    "        ordered_prediction_values[i] = All_Predictions[All_Predictions_time_change]\n",
    "        ordered_prediction_keys[i] = All_Predictions_time_change\n",
    "        ordered_prediction_keys_datetime[i] = time_change(T_datetime,d_d,date_time=true)[1]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    @views begin\n",
    "    for c in 1:length(data_list)\n",
    "        if c === 1 && length(data_list[c]) > 1\n",
    "            for d in 1:length(data_list[c])\n",
    "                data_list[c][d][5] = Dates.format(data_list[c][d][5], \"dd-u-yyyy HH:MM\")\n",
    "            end\n",
    "        elseif c !== 9 && length(data_list[c]) > 1\n",
    "            for d in 1:length(data_list[c])\n",
    "                data_list[c][d][5] = Dates.format(data_list[c][d][5], \"dd-u-yyyy HH:MM\")\n",
    "            end\n",
    "        elseif length(data_list[c]) > 1\n",
    "            for d in 1:length(data_list[c])\n",
    "                data_list[c][d][5] = Dates.format(data_list[c][d][5], \"dd U yyyy\")\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    end\n",
    "\n",
    "    return ordered_vols_values, ordered_vols_keys, ordered_price_values, ordered_price_keys, ordered_prediction_values, ordered_prediction_keys, data_list, ordered_prediction_values\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "loaded_data = load_files(\"Price_data\",\"Volume_data\");\n",
    "\n",
    "\n",
    "\n",
    "times = Float32[]\n",
    "days_of_week = Float32[]\n",
    "for c=1:length(loaded_data[2])\n",
    "    time = Dates.Time((DateTime(loaded_data[2][c],\"dd-u-yyyy HH:MM\")))\n",
    "    time_string = Dates.format(time, \"HH:MM\")\n",
    "    week_day = Dates.dayofweek((DateTime(loaded_data[2][c],\"dd-u-yyyy HH:MM\")))\n",
    "    push!(days_of_week,week_day)\n",
    "    \n",
    "    push!(times,parse(Int32,time_string[1:2])*60 + parse(Int32,time_string[4:5]))\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# williams_ad is an implemtation of the williams accumulation distribution technical indicator\n",
    "#\n",
    "\n",
    "function williams_ad(data)\n",
    "    WAD = zeros(Float32,length(data))\n",
    "    WAD[1] = data[1][9]\n",
    "    \n",
    "    for c=1:length(data)\n",
    "        if c > 1\n",
    "            prev_value = WAD[c-1]\n",
    "            prev_close = data[c-1][9]\n",
    "            if data[c][9] > prev_close\n",
    "                ad = data[c][9] - minimum([prev_close, data[c][8]])\n",
    "            elseif data[c][9] < prev_close\n",
    "                ad = data[c][9] - maximum([prev_close, data[c][7]])\n",
    "            else\n",
    "                ad = 0.\n",
    "            end\n",
    "            \n",
    "            WAD[c] = ad + prev_value\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return WAD\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Compiling the data matrix together from the  data\n",
    "#\n",
    "\n",
    "\n",
    "n = length(loaded_data[1])\n",
    "X = zeros(Float32,17,n)\n",
    "X[1,:] = loaded_data[3][1:n]\n",
    "X[2,:] = loaded_data[1][1:n]\n",
    "X[3,:] = [loaded_data[7][1][1:n][c][1] for c =1:length(loaded_data[7][1][1:n])]\n",
    "X[4,:] = [loaded_data[7][1][1:n][c][2] for c =1:length(loaded_data[7][1][1:n])]\n",
    "X[5,:] = [loaded_data[7][1][1:n][c][6] for c =1:length(loaded_data[7][1][1:n])]\n",
    "X[6,:] = [loaded_data[7][1][1:n][c][7] for c =1:length(loaded_data[7][1][1:n])]\n",
    "X[7,:] = [loaded_data[7][1][1:n][c][8] for c =1:length(loaded_data[7][1][1:n])]\n",
    "X[8,:] = williams_ad(loaded_data[7][1][1:n])\n",
    "difference_WAD = diff(williams_ad(loaded_data[7][1][1:n]))\n",
    "difference_WAD = vcat(Float32[0.f0], difference_WAD)\n",
    "X[9,:] = difference_WAD\n",
    "difference_open = diff([loaded_data[7][1][1:n][c][6] for c =1:length(loaded_data[7][1][1:n])])\n",
    "difference_open = vcat(Float32[0.f0], difference_open)\n",
    "X[10,:] = difference_open\n",
    "difference_high = diff([loaded_data[7][1][1:n][c][7] for c =1:length(loaded_data[7][1][1:n])])\n",
    "difference_high = vcat(Float32[0.f0], difference_high)\n",
    "X[11,:] = difference_high\n",
    "difference_low = diff([loaded_data[7][1][1:n][c][8] for c =1:length(loaded_data[7][1][1:n])])\n",
    "difference_low = vcat(Float32[0.f0], difference_low)\n",
    "X[12,:] = difference_low\n",
    "difference_close = diff(loaded_data[3][1:n])\n",
    "difference_close = vcat(Float32[0.f0], difference_close)\n",
    "X[13,:] = difference_close\n",
    "difference_volume = diff(loaded_data[1][1:n])\n",
    "difference_volume = vcat(Float32[0.f0], difference_volume)\n",
    "X[14,:] = difference_volume\n",
    "X[15,:] = loaded_data[8][1:n];\n",
    "X[16,:] = times[1:n]\n",
    "X[17,:] = days_of_week[1:n];\n",
    "\n",
    "difference_WAD = nothing\n",
    "difference_volume = nothing\n",
    "difference_open = nothing\n",
    "difference_high = nothing\n",
    "difference_low = nothing\n",
    "difference_close = nothing\n",
    "\n",
    "\n",
    "prices = loaded_data[3][1:n]\n",
    "sde_data = X\n",
    "scaling_range = 1:85000\n",
    "\n",
    "\n",
    "#\n",
    "# MV_standardize is an implementation of multivariate standardization which is done using the biweight\n",
    "# location and biweight midcovariance matrix instead of the mean and covariance matrix respectively.\n",
    "# This choice is done due to my experience that in financial data these statistics can offer more\n",
    "# information when they are used to standardize instead of mean and covariance due to lower sensitivity.\n",
    "#\n",
    "\n",
    "\n",
    "function MV_standardize(sde_data,scaling_range)\n",
    "    SL = transpose(sde_data)\n",
    "\n",
    "    \n",
    "    SL_T = transpose(SL)\n",
    "    SL_S = similar(SL_T)\n",
    "    ฮผ = [convert(Float32,ast_stats.biweight_location(SL[scaling_range,c])) for c in 1:size(sde_data)[1]]\n",
    "    ฮฃ = ast_stats.biweight_midcovariance(SL_T[:,scaling_range])\n",
    "    ฮฃ = [convert(Float32,ฮฃ[c,d]) for c=1:size(ฮฃ)[1], d=1:size(ฮฃ)[2]]\n",
    "\n",
    "    ฮฃ = ((ฮฃ)^(-1))^(1/2)\n",
    "\n",
    "\n",
    "    for c in 1:size(sde_data)[2]\n",
    "        SL_S[:,c] = ฮฃ*(SL_T[:,c] - ฮผ)\n",
    "    end\n",
    "    \n",
    "    return SL_S\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "sde_data = MV_standardize(sde_data,scaling_range)\n",
    "SDE = deepcopy(sde_data)\n",
    "\n",
    "\n",
    "_begin_ = 1\n",
    "_end_ = 12000\n",
    "delay = 30\n",
    "\n",
    "test_length = 2000\n",
    "\n",
    "\n",
    "y1 = [X[1,c] for c=_begin_+30:_end_+30] .- X[1,_begin_:_end_]\n",
    "Mii = -20#minimum(test_std)\n",
    "Mai = 20#maximum(test_std)\n",
    "\n",
    "y1 = [c/(Mai - Mii) for c in y1]#[(c - Mii)/(Mai - Mii) for c in y1]\n",
    "y1 = [Float32[y1[c]] for c=1:length(y1)];\n",
    "\n",
    "y1 = reshape([c[1] for c in y1],1,length(y1))\n",
    "\n",
    "\n",
    "seq = [[sde_data[d,c] for d=1:size(sde_data)[1]] for c=_begin_:_end_];\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the episodic recall LSTM architecture\n",
    "\n",
    "<font size=\"3.5\">Here I create the struct and functions for the episodic recall LSTM to be compatible with Julia's Flux neural network library for ease of training. I also create the DND and functions related to tracking its status as well as adding, removing, and retrieving cell states from it. Special care is made to ensure the DND is in fact differentiable so it can be trained with the gradient descent algorithms which utilise Flux's exact gradients via automatic differention.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a base LSTM cell struct which will hold base LSTM cell info\n",
    "struct LSTMMVCell{A,V,S}\n",
    "    Wi::A\n",
    "    Wh::A\n",
    "    b::V\n",
    "    state0::S\n",
    "end\n",
    "\n",
    "# Defining a constructor function that intantiates a layer of the episoidc recall LSTM\n",
    "function LSTMMVCell(in::Integer, out::Integer;init = Flux.glorot_uniform)\n",
    "    cell = LSTMMVCell(init(out * 5, in), init(out * 5, out), zeros(Float32,out * 5), (zeros(Float32,out,1), zeros(Float32,out,1)))\n",
    "    cell.b[Flux.gate(out, 2)] .= 1\n",
    "    return cell\n",
    "end\n",
    "\n",
    "# Variables and dictionaries that are used for the DND\n",
    "# c_dict is the DND while the rest are used to keep track of which layers of the network are storing cell\n",
    "# states in the DND\n",
    "num_cells = 1\n",
    "c_dict = Dict(a => Dict{Array,Array}() for a=1:num_cells)\n",
    "cell_memory_tracker = Dict{Int,Int}(a => 0 for a=1:num_cells)\n",
    "cell_track = 0\n",
    "\n",
    "# Kernel function used to measure similarity to other environment states\n",
    "function kernel(h::Array{Float32,1},hแตข::Dict{Int64,Array{Float32,1}};ฮด=0.001f0)::Tuple{Array{Float32,1},Dict{Float32,Array}}\n",
    "    HH = [hแตข[d] for d=1:length(hแตข)]\n",
    "    distances = 1.f0./(map(r::Array -> squaresum(h - r),HH).+ ฮด)\n",
    "    Di = Dict{Float32,Array}(zip(distances,HH))\n",
    "    return distances,Di\n",
    "end\n",
    "squaresum(x::Array)::Float32 = sum(x.^2)\n",
    "\n",
    "\n",
    "# Precalculated nearest 50 environment states under the kernel function used in place of p-nearest neighbours\n",
    "# at the moment, currently working on other methods of retrieval\n",
    "c_history = Dict()\n",
    "for c=1:7000\n",
    "    distances,Di = kernel(seq[c],Dict(d => seq[d] for d=1:length(seq[1:c])))\n",
    "    sort_ = length(distances) > 50 ? partialsort(distances,1:50) : sort(distances)\n",
    "    c_history[c] = (sort_,[Di[sort_[d]] for d=1:length(sort_)])\n",
    "end\n",
    "c_history = [c_history[c] for c=1:length(c_history)];\n",
    "\n",
    "\n",
    "# Function used to retrieve the cell states from the DND\n",
    "function get_c(x,c_size)\n",
    "    global c_dict, cell_track, c_history\n",
    "    cell_track::Int64 += 1\n",
    "    current_cell = (cell_track%num_cells + 1)::Int64\n",
    "    if cell_track::Int64 > num_cells::Int64\n",
    "        memory_retrieval = c_history[cell_track-1]\n",
    "        \n",
    "        nearest_cell_states = [c_dict[current_cell][memory_retrieval[2][i]] for i=1:length(memory_retrieval[2])]\n",
    "\n",
    "        return sum((memory_retrieval[1].*nearest_cell_states)./sum(memory_retrieval[1]))\n",
    "            \n",
    "    else\n",
    "        return zeros(Float32,c_size)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Function used to store the cell states in the DND\n",
    "function store_c(c,x)\n",
    "    global c_dict, c_keys, cell_memory_tracker\n",
    "    current_cell = cell_track%num_cells + 1\n",
    "    c_dict[current_cell][x] = c\n",
    "    \n",
    "    cell_memory_tracker[current_cell] += 1\n",
    "end\n",
    "\n",
    "# Function used on an episodic recall LSTM layer which provides the architecture\n",
    "function (m::LSTMMVCell{A,V,<:NTuple{2,AbstractMatrix{T}}})((h, c), x::Union{AbstractVecOrMat{T},Flux.OneHotArray}) where {A,V,T}\n",
    "    b, o = m.b, size(h, 1)\n",
    "    g = m.Wi*x .+ m.Wh*h .+ b\n",
    "\n",
    "    input = ฯ.(Flux.gate(g, o, 1))\n",
    "    forget = ฯ.(Flux.gate(g, o, 2))\n",
    "    cell = tanh.(Flux.gate(g, o, 3))\n",
    "\n",
    "    # In addition to regular LSTMs, the following reinstatement gate is used to introduce the recalled\n",
    "    # cell states\n",
    "    reinstatement = ฯ.(Flux.gate(g, o, 4))\n",
    "    \n",
    "    output = ฯ.(Flux.gate(g, o, 5))\n",
    "    \n",
    "\n",
    "    # Recalling the cell states\n",
    "    c_in = get_c(x,size(c))\n",
    "    \n",
    "\n",
    "    c = forget .* c .+ input .* cell .+ reinstatement .* tanh.(c_in)\n",
    "    # Storing the cell states\n",
    "    store_c(c,x)\n",
    "    \n",
    "    hโฒ = output .* tanh.(c)\n",
    "    sz = size(x)\n",
    "    return (hโฒ, c), reshape(hโฒ, :, sz[2:end]...)\n",
    "end\n",
    "\n",
    "\n",
    "# Completing the setup of the episodic recall LSTM struct and functions to be compatible with Flux\n",
    "@Flux.functor LSTMMVCell\n",
    "\n",
    "LSTMMV(a...; ka...) = Recur(LSTMMVCell(a...; ka...))\n",
    "Recur(m::LSTMMVCell) = Flux.Recur(m, m.state0)\n",
    "\n",
    "# A modified cell reset function used to reset the standard LSTM memory\n",
    "function MV_reset!(L;mem_reset=true)\n",
    "    if mem_reset\n",
    "        global c_dict, cell_track, c_keys, cell_memory_tracker\n",
    "        c_dict = Dict(1 => Dict{Array,Array}())\n",
    "        cell_memory_tracker = Dict{Int,Int}(1 => 0)\n",
    "        cell_track = 0\n",
    "    end\n",
    "    Flux.reset!(L)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular and Episodic LSTM comparison\n",
    "\n",
    "<font size=\"3.5\">Having implemented the episodic LSTM I made an example comparison between it and a regular LSTM. They were both trained from the same initially randomised weights (without the randomised reinstatement weights for the regular LSTM) and for the same number of iterations over the same small dataset of 850 minutes of minute gold futures data from COMEX GCZ2020. We compare the results of training the networks to predict the difference in close price of the instrument 30 minutes in the future.\n",
    "\n",
    "The networks were trained on data from July 27, 2020 at 06:21 and July 27, 2020 at 21:35. They are then tested on 1000 minutes of data taken slightly later from July 28, 2020 at 00:05 to July 28, 2020 at 16:46. The figures are the results of this test.\n",
    "\n",
    "It should be noted that both networks are not deep at 2 layers nor do they have many neurons, they however sufficiently manage to display the differences in the capability of regular and episodic LSTMs also noticed in larger networks which I have trained and tested on the same data.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small 2 layer LSTM with initial layer having episodic memory\n",
    "episodic_memory_network = Chain(LSTMMV(17,170),LSTM(170,1))\n",
    "\n",
    "# Regular 2 layer LSTM\n",
    "standard_network = Chain(LSTM(17,170),LSTM(170,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](LSTM_gif.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](LSTM_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3.5\">It can be seen that the episodic LSTM produced much tighter predictions with lower variation after being trained from the very small 850 minutes of data. </font>"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
