{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, Flux, Statistics, Dates, TimeZones, PyCall, Plots\n",
    "ass = pyimport(\"astropy.stats\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Importing and Cleaning\n",
    "\n",
    "<font size=\"3.5\">The data used for the training of the reinforcement learning agent seen below is minute gold futures data from COMEX GCZ2020 between September 29, 2020 at 16:43 to October 6, 2020 at 04:06.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "118430\n",
      "5000\n",
      "53223\n",
      "2000\n",
      "21877\n",
      "1000\n",
      "10915\n",
      "334\n",
      "3904\n",
      "167\n",
      "1991\n",
      "84\n",
      "1072\n",
      "42\n",
      "578\n",
      "0\n",
      "83\n",
      "data_set118385\n",
      "2020-07-27T06:21:002020-11-26T11:00:00\n",
      "vols: 118385\n",
      "vols: 118385data_list[1]118385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11×90000 Matrix{Float32}:\n",
       " -0.614298   -0.306866  -0.0895664  …  -1.09971    -1.16044   -1.08443\n",
       " -0.955186   -0.898524  -0.898109       0.832313   -0.111926  -0.193491\n",
       " -0.253459    0.503066   1.03196       -0.253508   -0.460548  -0.36896\n",
       "  2.2876      1.66966    1.30798       -0.131907   -0.101853  -0.266856\n",
       " -2.10481    -2.11333   -2.11156        1.93963     1.92907    1.92256\n",
       " -0.0191491  -1.37407    0.336888   …  -0.0983289   0.168835   0.536094\n",
       " -0.0734421   0.547043   0.599537      -1.08401     0.519918   0.458255\n",
       "  0.177579    0.259612   0.157577       1.65432    -1.73603    0.151454\n",
       " -0.857587   -1.2148    -1.445          0.292013    0.366199   0.30841\n",
       " -0.769535   -0.765095  -0.762582       1.28852     1.25921    1.25473\n",
       " -1.01361    -1.00998   -1.00686    …  -0.196495   -0.174816  -0.162342"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# The functions time_change, find_date, data_ret, and line_prediction are functions needed\n",
    "# and solely use for the importation and cleaning of the financial data I am using.\n",
    "#\n",
    "\n",
    "\n",
    "function time_change(T,mins;date_time=false)\n",
    "    if date_time === false\n",
    "        date_time = DateTime(T,\"dd-u-yyyy HH:MM\") + Minute(mins)\n",
    "        time = Dates.format(date_time, \"dd-u-yyyy HH:MM\")\n",
    "    \n",
    "        return date_time, time\n",
    "    else\n",
    "        date_time = T + Minute(mins)\n",
    "        time = Dates.format(date_time, \"dd-u-yyyy HH:MM\")\n",
    "        \n",
    "        return date_time, time\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function find_date(elements, value; days=false)\n",
    "    \n",
    "    \n",
    "    left = 1\n",
    "    right = length(elements)\n",
    "\n",
    "    first = 1\n",
    "    last = length(elements)\n",
    "        \n",
    "    while left <= right\n",
    "        middle = (left + right) ÷ 2\n",
    "        middle_element = elements[middle][5]\n",
    "        \n",
    "        if middle != first && middle != last\n",
    "            left_element = elements[middle-1][5]\n",
    "            right_element = elements[middle+1][5]\n",
    "\n",
    "            if middle_element == value\n",
    "                return middle_element,middle\n",
    "            end\n",
    "            if left_element == value\n",
    "                return left_element,middle-1\n",
    "            end\n",
    "            if right_element == value\n",
    "                return right_element,middle+1\n",
    "            end\n",
    "            if middle - left <= 1 && right - middle <= 1\n",
    "                times_under_consideration = Dict()\n",
    "                datetime_triple = [(left_element,left_element,middle - 1),\n",
    "                                   (middle_element,middle_element,middle),\n",
    "                                   (right_element,right_element,middle + 1)]\n",
    "                for c in 1:length(datetime_triple)\n",
    "                    if value > datetime_triple[c][1]\n",
    "                        times_under_consideration[value - datetime_triple[c][1]] = (datetime_triple[c][2],datetime_triple[c][3])\n",
    "                    end\n",
    "                end\n",
    "                if length(times_under_consideration) > 0\n",
    "                    to_be_returned = times_under_consideration[minimum(keys(times_under_consideration))] \n",
    "                    return to_be_returned\n",
    "                end\n",
    "            end\n",
    "        elseif middle == first || middle == last\n",
    "            if value <= middle_element\n",
    "                return middle_element, middle\n",
    "            end\n",
    "            if middle_element <= value\n",
    "                return middle_element, middle\n",
    "            end\n",
    "        end\n",
    "                \n",
    "        if middle_element < value\n",
    "            left = middle + 1\n",
    "        elseif middle_element > value\n",
    "            right = middle - 1\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function data_ret(T_date;data_location=data_array,A=0,price_grab=false)\n",
    "    \n",
    "    Keys = []\n",
    "    Data = []\n",
    "    \n",
    "    @views begin\n",
    "    if price_grab === true\n",
    "        time, index = find_date(data_location[1],T_date)\n",
    "        push!(Keys,(1,index))\n",
    "        push!(Data,data_location[1][index])\n",
    "        return Data[1][9]\n",
    "    end\n",
    "    \n",
    "    for c in 1:length(data_location)\n",
    "        if c !== 9\n",
    "            time, index = find_date(data_location[c],T_date)\n",
    "            push!(Keys,(c,index))\n",
    "            push!(Data,data_location[c][index])\n",
    "        else\n",
    "            time, index = find_date(data_location[c],T_date;days=true)\n",
    "            push!(Keys,(c,index))\n",
    "            push!(Data,data_location[c][index])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    dP_dt = [Data[c][2] for c in 1:length(Data)]\n",
    "    dW_dt = [Data[c][1] for c in 1:length(Data)]\n",
    "    d2P_dt2 = []\n",
    "    d2W_dt2 = []\n",
    "    end\n",
    "        \n",
    "    if A !== 0\n",
    "        A += 1\n",
    "        y_list = [(data_location[1][Keys[1][2]+-c][7]+data_location[1][Keys[1][2]+-c][8])/2 for c in 1:A]\n",
    "        return dP_dt, dW_dt, d2P_dt2, d2W_dt2, Keys, Data, y_list\n",
    "    end\n",
    "\n",
    "    return dP_dt, dW_dt, d2P_dt2, d2W_dt2, Keys, Data\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function line_prediction(t,dP_dt,dW_dt;d2P_dt2=0,d2W_dt2=0,gamma=1,kappa=0,Price=Price)\n",
    "    \n",
    "    predictions_difference = []\n",
    "    for x in eachindex(dP_dt)\n",
    "        push!(predictions_difference,((dP_dt[x]*(0-(t/periods[x]))+((0-(t/periods[x]))*(dW_dt[x]-dP_dt[x])*gamma) + Price)))\n",
    "    end\n",
    "    \n",
    "        \n",
    "    total = []\n",
    "    \n",
    "    for c in eachindex(predictions_difference)\n",
    "        if abs(t-periods[c]) !== 0\n",
    "            push!(total,(predictions_difference[c]/abs(t-periods[c])))\n",
    "        else\n",
    "            push!(total,predictions_difference[c])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    average = mean(total)\n",
    "    \n",
    "    for c in eachindex(total)\n",
    "        total[c] = (total[c]/average)*predictions_difference[c]\n",
    "    end\n",
    "\n",
    "    prediction = mean(total)\n",
    "    \n",
    "    return prediction\n",
    "end\n",
    "\n",
    "\n",
    "periods = [1,2,5,10,30,60,120,240,1440]\n",
    "\n",
    "\n",
    "#\n",
    "# The function load_files loads the data files I am using, cleans the data, and puts it in an appropriate\n",
    "# format for use in training.\n",
    "#\n",
    "\n",
    "function load_files(s,v)\n",
    "\n",
    "    data_list = [[], [], [], [], [], [], [], [], []]\n",
    "\n",
    "    for aa in 1:9\n",
    "        storage_file = s*\"indicator_data_\"*string(aa)*\".csv\"\n",
    "        \n",
    "        io = open(storage_file, \"r\")\n",
    "        lines = readlines(io)\n",
    "        split_lines = [split(lines[x],\",\") for x in 1:length(lines)]\n",
    "\n",
    "        current_data_times = Set([split_lines[x][5] for x in 1:length(split_lines)])\n",
    "\n",
    "        if aa != 9\n",
    "            transform = Dict(1=>x->parse(Float64,x),2=>x->parse(Float64,x),3=>x->parse(Float64,x),4=>x->parse(Int8,x),5=>x->DateTime(x,\"dd-u-yyyy HH:MM\"),6=>x->parse(Float64,x),7=>x->parse(Float64,x),\n",
    "                            8=>x->parse(Float64,x),9=>x->parse(Float64,x),10=>x->x,11=>x->x)\n",
    "        else\n",
    "            transform = Dict(1=>x->parse(Float64,x),2=>x->parse(Float64,x),3=>x->parse(Float64,x),4=>x->parse(Int8,x),5=>x->DateTime(x,\"dd U yyyy\"),6=>x->parse(Float64,x),7=>x->parse(Float64,x),\n",
    "                            8=>x->parse(Float64,x),9=>x->parse(Float64,x),10=>x->x,11=>x->x)\n",
    "        end\n",
    "\n",
    "        @views begin\n",
    "        current_data = [[transform[x](split_lines[c][x]) for x in 1:11] for c in 1:length(split_lines)]\n",
    "        current_data = unique(current_data)\n",
    "        end\n",
    "        close(io)\n",
    "        #end\n",
    "        println(length(current_data))\n",
    "\n",
    "        storage_file = s*\"indicator_storage_aggregate_\"*string(aa)*\".csv\"\n",
    "        io = open(storage_file, \"r\")\n",
    "        lines = readlines(io)\n",
    "        split_lines = [split(lines[x],\",\") for x in 1:length(lines)]\n",
    "\n",
    "\n",
    "        @views begin\n",
    "        aggregate_data = [[transform[x](split_lines[c][x]) for x in 1:11] for c in 1:length(split_lines) if split_lines[c][5] ∉ current_data_times]\n",
    "        end\n",
    "        append!(current_data,aggregate_data)\n",
    "        close(io)\n",
    "\n",
    "        println(length(current_data))\n",
    "\n",
    "        data_list[aa] = current_data\n",
    "    end\n",
    "\n",
    "    \n",
    "    volume_file = v\n",
    "\n",
    "    io = open(volume_file, \"r\")\n",
    "    lines = readlines(io)\n",
    "    split_lines = [split(lines[x],\",\") for x in 2:length(lines)]\n",
    "    times = [split_lines[x][1] for x in 1:length(split_lines)]\n",
    "    volumes = []\n",
    "    \n",
    "    try\n",
    "        volumes = [parse(Int, split_lines[x][6]) for x in 1:length(split_lines)]\n",
    "    catch ArgumentError\n",
    "        volumes = [convert(Int,parse(Float64, split_lines[x][6])) for x in 1:length(split_lines)]\n",
    "    end\n",
    "    close(io)\n",
    "\n",
    "    try\n",
    "        DateTime(times[1],\"dd-u-yyyy HH:MM\")\n",
    "    catch\n",
    "        times = [Dates.format(astimezone(ZonedDateTime(DateTime(replace(times[x],\"Z\"=>\"\"), \"y-m-dTH:M:S\"), tz\"UTC\"), tz\"America/Toronto\"),\"dd-u-yyyy HH:MM\") for x in 1:length(times)]\n",
    "    end\n",
    "\n",
    "    times = reverse(times)\n",
    "    volumes = reverse(volumes)\n",
    "    times_as_datetime = [DateTime(times[x],\"dd-u-yyyy HH:MM\") for x in 1:length(times)]\n",
    "\n",
    "    data_list[1] = [data_list[1][c] for c in 1:length(data_list[1]) if data_list[1][c][5] ∈ times_as_datetime]\n",
    "\n",
    "    @views begin\n",
    "    for c in 1:length(data_list)\n",
    "        if c != 9 && length(data_list[c]) > 1\n",
    "            data_list[c] = sort(data_list[c],by= x -> x[5])\n",
    "        elseif length(data_list[c]) > 1\n",
    "            data_list[c] = sort(data_list[c],by= x -> x[5])\n",
    "        end\n",
    "    end\n",
    "    end\n",
    "\n",
    "\n",
    "\n",
    "    data_datetimes = [data_list[1][c][5] for c in 1:length(data_list[1])]\n",
    "\n",
    "    vols = Dict()\n",
    "    data_strings = [Dates.format(data_list[1][c][5], \"dd-u-yyyy HH:MM\") for c in 1:length(data_list[1])]\n",
    "    data_set = Set(data_strings)\n",
    "\n",
    "    @views begin\n",
    "    for c in 1:length(volumes)\n",
    "        if times[c] in data_set\n",
    "            vols[times[c]] = volumes[c]\n",
    "        end\n",
    "    end\n",
    "    end\n",
    "\n",
    "\n",
    "    println(\"data_set\",length(data_set))\n",
    "\n",
    "    println(data_list[1][1][5],data_list[1][length(data_list[1])][5])\n",
    "\n",
    "    vols_tuple = [(times[c],volumes[c]) for c in 1:length(volumes) if times[c] ∈ data_set]\n",
    "    println(\"vols: \",length(vols))\n",
    "\n",
    "    times_as_datetime = nothing\n",
    "    data_set = nothing\n",
    "    times = nothing\n",
    "    volumes = nothing\n",
    "\n",
    "    looking = 50\n",
    "    d_d = 25\n",
    "\n",
    "    All_Prices = Dict()\n",
    "    All_Predictions = Dict()\n",
    "\n",
    "    pred_deviation = Dict()\n",
    "    Price_deviation = Dict()\n",
    "    difference_deviation = Dict()\n",
    "\n",
    "    gamma = -15\n",
    "\n",
    "    println(\"vols: \",length(vols),\"data_list[1]\",length(data_list[1]))\n",
    "\n",
    "    ordered_vols_values = similar(data_list[1],Int64)\n",
    "    ordered_vols_keys = similar(data_list[1],String)\n",
    "\n",
    "    for i in 1:length(data_list[1])\n",
    "        ordered_vols_keys[i] = vols_tuple[i][1]\n",
    "        ordered_vols_values[i] = vols_tuple[i][2]\n",
    "    end\n",
    "\n",
    "    ordered_price_values = similar(data_list[1],Float64)\n",
    "    ordered_price_keys = similar(data_list[1],String)\n",
    "    ordered_price_keys_datetime = similar(data_list[1],DateTime)\n",
    "\n",
    "    ordered_prediction_values = similar(data_list[1],Float64)\n",
    "    ordered_prediction_keys = similar(data_list[1],String)\n",
    "    ordered_prediction_keys_datetime = similar(data_list[1],DateTime)\n",
    "\n",
    "    for i in 1:length(data_list[1])\n",
    "       @views begin\n",
    "        T = data_strings[i]\n",
    "        T_datetime = data_datetimes[i]\n",
    "        data_T = data_ret(T_datetime;data_location=data_list)\n",
    "        All_Predictions_time_change = time_change(T_datetime,d_d,date_time=true)[2]\n",
    "\n",
    "        All_Prices[T] = data_T[6][1][9]\n",
    "        dW_dt_d = data_T[1]\n",
    "        dP_dt_d = data_T[2]\n",
    "\n",
    "        All_Predictions[All_Predictions_time_change] = line_prediction(d_d,dP_dt_d,dW_dt_d;Price=All_Prices[T],gamma=gamma)\n",
    "\n",
    "        ordered_price_values[i] = data_T[6][1][9]\n",
    "        ordered_price_keys[i] = T\n",
    "        ordered_price_keys_datetime[i] = T_datetime\n",
    "\n",
    "        ordered_prediction_values[i] = All_Predictions[All_Predictions_time_change]\n",
    "        ordered_prediction_keys[i] = All_Predictions_time_change\n",
    "        ordered_prediction_keys_datetime[i] = time_change(T_datetime,d_d,date_time=true)[1]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    @views begin\n",
    "    for c in 1:length(data_list)\n",
    "        if c === 1 && length(data_list[c]) > 1\n",
    "            for d in 1:length(data_list[c])\n",
    "                #transformed = Dates.format(data_list[c][d][5], \"dd-u-yyyy HH:MM\")\n",
    "                #data_list_strings_and_datetimes[transformed] = data_list[c][d][5]\n",
    "                data_list[c][d][5] = Dates.format(data_list[c][d][5], \"dd-u-yyyy HH:MM\")\n",
    "            end\n",
    "        elseif c !== 9 && length(data_list[c]) > 1\n",
    "            for d in 1:length(data_list[c])\n",
    "                data_list[c][d][5] = Dates.format(data_list[c][d][5], \"dd-u-yyyy HH:MM\")\n",
    "            end\n",
    "        elseif length(data_list[c]) > 1\n",
    "            for d in 1:length(data_list[c])\n",
    "                data_list[c][d][5] = Dates.format(data_list[c][d][5], \"dd U yyyy\")\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    end\n",
    "\n",
    "    return ordered_vols_values, ordered_vols_keys, ordered_price_values, ordered_price_keys, ordered_prediction_values, ordered_prediction_keys, data_list, ordered_prediction_values\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "loaded_data = load_files(\"Price_data\",\"Volume_data\");\n",
    "\n",
    "\n",
    "\n",
    "times = Float32[]\n",
    "days_of_week = Float32[]\n",
    "for c=1:length(loaded_data[2])\n",
    "    time = Dates.Time((DateTime(loaded_data[2][c],\"dd-u-yyyy HH:MM\")))\n",
    "    time_string = Dates.format(time, \"HH:MM\")\n",
    "    week_day = Dates.dayofweek((DateTime(loaded_data[2][c],\"dd-u-yyyy HH:MM\")))\n",
    "    push!(days_of_week,week_day)\n",
    "    \n",
    "    push!(times,parse(Int32,time_string[1:2])*60 + parse(Int32,time_string[4:5]))\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# williams_ad is an implemtation of the williams accumulation distribution technical indicator\n",
    "#\n",
    "\n",
    "function williams_ad(data)\n",
    "    WAD = zeros(Float32,length(data))\n",
    "    WAD[1] = data[1][9]\n",
    "    \n",
    "    for c=1:length(data)\n",
    "        if c > 1\n",
    "            prev_value = WAD[c-1]\n",
    "            prev_close = data[c-1][9]\n",
    "            if data[c][9] > prev_close\n",
    "                ad = data[c][9] - minimum([prev_close, data[c][8]])\n",
    "            elseif data[c][9] < prev_close\n",
    "                ad = data[c][9] - maximum([prev_close, data[c][7]])\n",
    "            else\n",
    "                ad = 0.\n",
    "            end\n",
    "            \n",
    "            WAD[c] = ad + prev_value\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return WAD\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Compiling the data matrix together from the  data\n",
    "#\n",
    "\n",
    "\n",
    "n = length(loaded_data[1])\n",
    "X = zeros(Float32,11,n)\n",
    "X[1,:] = loaded_data[3][1:n]\n",
    "X[2,:] = loaded_data[1][1:n]\n",
    "X[3,:] = [loaded_data[7][1][1:n][c][1] for c =1:length(loaded_data[7][1][1:n])]\n",
    "X[4,:] = [loaded_data[7][1][1:n][c][2] for c =1:length(loaded_data[7][1][1:n])]\n",
    "X[5,:] = williams_ad(loaded_data[7][1][1:n])\n",
    "difference_WAD = diff(williams_ad(loaded_data[7][1][1:n]))\n",
    "difference_WAD = vcat(Float32[0.f0], difference_WAD)\n",
    "X[6,:] = difference_WAD\n",
    "difference_price = diff(loaded_data[3][1:n])\n",
    "difference_price = vcat(Float32[0.f0], difference_price)\n",
    "X[7,:] = difference_price\n",
    "difference_volume = diff(loaded_data[1][1:n])\n",
    "difference_volume = vcat(Float32[0.f0], difference_volume)\n",
    "X[8,:] = difference_volume\n",
    "X[9,:] = loaded_data[8][1:n];\n",
    "X[10,:] = times[1:n]\n",
    "X[11,:] = days_of_week[1:n];\n",
    "\n",
    "difference_WAD = nothing\n",
    "difference_price = nothing\n",
    "difference_volume = nothing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prices = loaded_data[3][1:n]\n",
    "sde_data = X\n",
    "scaling_range = 1:85000\n",
    "\n",
    "\n",
    "#\n",
    "# MV_standardize is an implementation of multivariate standardization which is done using the biweight\n",
    "# location and biweight midcovariance matrix instead of the mean and covariance matrix respectively.\n",
    "# This choice is done due to my experience that in financial data these statistics can offer more\n",
    "# information when they are used to standardize instead of mean and covariance due to lower sensitivity.\n",
    "#\n",
    "\n",
    "\n",
    "function MV_standardize(sde_data,scaling_range)\n",
    "    SL = transpose(sde_data)\n",
    "\n",
    "    \n",
    "    SL_T = transpose(SL)\n",
    "    SL_S = similar(SL_T)\n",
    "    μ = [convert(Float32,ass.biweight_location(SL[scaling_range,c])) for c in 1:size(sde_data)[1]]\n",
    "    Σ = ass.biweight_midcovariance(SL_T[:,scaling_range])\n",
    "    Σ = [convert(Float32,Σ[c,d]) for c=1:size(Σ)[1], d=1:size(Σ)[2]]\n",
    "\n",
    "    Σ = ((Σ)^(-1))^(1/2)\n",
    "\n",
    "\n",
    "    for c in 1:size(sde_data)[2]\n",
    "        SL_S[:,c] = Σ*(SL_T[:,c] - μ)\n",
    "    end\n",
    "    \n",
    "    return SL_S\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "sde_data = MV_standardize(sde_data,scaling_range,Σ=Σ,μ=μ)\n",
    "SDE = deepcopy(sde_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the networks and adjusting training parameters\n",
    "\n",
    "Below is an LSTM helper network trained on COMEX GCZ2020 data before the data used here to train the reinforcement learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Here we create a 6 layer LSTM neural network that will augment the data imputs to the\n",
    "# reinforcement learner as a further state variable which has been trained on short term\n",
    "# prediction of price fluctuations. After creation we load the trained weights into the \n",
    "# network.\n",
    "#\n",
    "\n",
    "\n",
    "m = Chain(LSTM(11, 300), LSTM(300,200), LSTM(200,100), LSTM(100,30), LSTM(30,100), Dense(100, 2))\n",
    "\n",
    "io = open(\"m_weights\", \"r\");\n",
    "a = readline(io)\n",
    "close(io)\n",
    "\n",
    "marks = [];\n",
    "\n",
    "\n",
    "for c=1:length(a)\n",
    "    if a[c] === '.'\n",
    "        if c-2 >= 1\n",
    "            if a[c-2] === '-'\n",
    "                push!(marks,c-2)\n",
    "            else\n",
    "                push!(marks,c-1)\n",
    "            end\n",
    "        else\n",
    "            push!(marks,c-1)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "W = zeros(Float32,length(marks));\n",
    "\n",
    "for c=1:length(marks)-1\n",
    "    W[c] = parse(Float32,a[marks[c]:marks[c+1]-1])\n",
    "end\n",
    "\n",
    "i = 0\n",
    "for a=1:length(Flux.params(m))\n",
    "    if length(size(Flux.params(m)[a])) > 1\n",
    "        for c=1:size(Flux.params(m)[a])[1], d=1:size(Flux.params(m)[a])[2]\n",
    "            i += 1\n",
    "            Flux.params(m)[a][c,d] = W[i]\n",
    "        end\n",
    "    else\n",
    "        for c=1:length(Flux.params(m)[a])\n",
    "            i += 1\n",
    "            Flux.params(m)[a][c] = W[i]\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "seq = [[sde_data[d,c] for d=1:size(sde_data)[1]] for c=scaling_range];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Within this model I have chosen these 5 layer LSTM networks for the state value function and \n",
    "# to act as an initial input to the preference function \n",
    "#\n",
    "\n",
    "\n",
    "Preference_network = Chain(\n",
    "    LSTM(19,190),\n",
    "    LSTM(190,90),\n",
    "    LSTM(90,40),\n",
    "    LSTM(40,50),\n",
    "    Dense(50,3));\n",
    "State_value_function = Chain(\n",
    "    LSTM(19,130),\n",
    "    LSTM(130,70),\n",
    "    LSTM(70,35),\n",
    "    LSTM(35,40),\n",
    "    Dense(40,1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Preference_function (generic function with 1 method)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# This is the full preference function which will be used as part of the policy. In addition\n",
    "# to the input from the Preference_network, specific attention is paid here to the state inputs\n",
    "# provided by the 6 layer LSTM price prediction network \"m\"\n",
    "#\n",
    "\n",
    "function Preference_function(s)\n",
    "    \n",
    "    # current and previous price predictions of the network m\n",
    "    current = Float32[s[1],s[2]]\n",
    "    previous = Float32[s[3],s[4]]\n",
    "    # history is an exponential moving average of previous preferences to buy and sell\n",
    "    history = Float32[s[5],s[6]]\n",
    "    \n",
    "    # Values used in the calculation of the exponential moving average\n",
    "    r_1 = 0.f0\n",
    "    r_2 = 0.f0\n",
    "    \n",
    "    \n",
    "    # Drawing the initial preference input\n",
    "    Pref = abs.(Preference_network(s))\n",
    "    \n",
    "    \n",
    "    # New values for the exponential moving average are based on the predictions of m \n",
    "    # magnified by exponentiation depending on the preference to do something (Pref[3]) \n",
    "    # as opposed to nothing\n",
    "    \n",
    "    if current[1] - previous[1] > 0\n",
    "        r_1 += (current[1] - previous[1])^(1.5+15*Pref[3])\n",
    "    else\n",
    "        r_2 += (-(current[1] - previous[1]))^(1.5+15*Pref[3])\n",
    "    end\n",
    "\n",
    "\n",
    "    if current[2] - previous[2] > 0\n",
    "        r_1 += ((current[2] - previous[2])/2)^(1.5+15*Pref[3])\n",
    "        else \n",
    "        r_2 += (-((current[2] - previous[2])/2))^(1.5+15*Pref[3])\n",
    "    end\n",
    "\n",
    "    \n",
    "    # Weights of the exponential moving average\n",
    "    weights = Float32[0.6f0,0.4f0]\n",
    "    \n",
    "    # Calculation of new EMA value\n",
    "    history = weights[1].*history + weights[2].*Float32[r_1,r_2]\n",
    "\n",
    "    # If there is agreement in the EMA of buying and selling and the current preference to buy and sell\n",
    "    # then we choose to buy amplified by the preference to do something \n",
    "    if history[1] > Pref[1] && history[2] < Pref[2]\n",
    "        choice = Float32[Pref[1]+Pref[3],0.,0.]\n",
    "    elseif history[1] < Pref[1] && history[2] > Pref[2]\n",
    "        choice = Float32[0.,Pref[2]+Pref[3],0.]\n",
    "    else\n",
    "        # If there is isn't agreement with current preference and the history we channel the \n",
    "        # preference for doing anything into doing nothing as this preference is not valid\n",
    "        choice = Float32[0.,0.,Pref[1]+Pref[2]+Pref[3]]\n",
    "    end\n",
    "    \n",
    "    \n",
    "    return choice, history, Pref\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Policy_choice (generic function with 1 method)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the policy function used to choose the action, but currently I am exploring \n",
    "# the effects of making it deterministic in the maximum value of the preference function\n",
    "# (this can be seen by how the output of the preference function is constructed)\n",
    "# as opposed to using softmax when making the action choice\n",
    "\n",
    "function Policy_choice(s)\n",
    "    preference, history, P = Preference_function(s)\n",
    "    return findall(x->x == maximum(preference),preference)[1], history, P\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "π_ (generic function with 1 method)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This gives the softmax probabilities which are used later during learning\n",
    "function π_(a,s)\n",
    "    M = Flux.softmax(Preference_network(s))\n",
    "    return M[a]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Any[]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting data used to train, each data point is on the minute timescale\n",
    "\n",
    "_begin_ = 62000\n",
    "_end_ = 67000\n",
    "\n",
    "D = prices[_begin_:_end_];\n",
    "D = [convert(Float32,c) for c in D];\n",
    "\n",
    "value_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# These are the parameters of the actor-critic with eligibility traces (continuing) algorithm\n",
    "#\n",
    "\n",
    "# Step size of the gradient descent of the state value function network and preference function network\n",
    "# respectively\n",
    "η_w = 0.00018\n",
    "η_θ = 0.00009\n",
    "\n",
    "# Weight of the new componenet of the expoenential moving average of the reward\n",
    "η_R̂ = 0.025\n",
    "\n",
    "# Weights on the Eligibility traces\n",
    "λ_w = 0.11\n",
    "λ_θ = 0.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "\n",
    "<font size=\"3.5\"> In this section I have implemented the continuous verion of actor-critic with eligibility traces. I have previously implemented and used REINFORCE, REINFORCE with baseline, and Q-learning for episodic tasks such as mazes and the multi-armed bandit. I am planning on implementing and testing deep double Q learning to compare it to continuous actor-critic with eligibility traces on non-episodic tasks like participation in financial markets.\n",
    "\n",
    "The following results were obtained after 68 training iterations with the above training parameters.\n",
    "    \n",
    "There are many planned improvements for this, among them are training for risk assessment and leverage.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-15.963928           1000\n",
      "33.710205           2000\n",
      "133.79187           3000\n",
      "227.23633           4000\n",
      "186.3241           5000\n",
      "Return: 186.3241          Money: 1186.3240828023463\n",
      "1\n",
      "67000\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "\n",
    "# This is my current implementation of the actor-critic with eligibility traces (continuing) algorithm\n",
    "# for on-line continuous learning of the reinforcement learning agent which can found on pg.333 of \n",
    "# Sutton and Barto\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "# Arrays to store interesting quantities for viewing later\n",
    "state_value_function_history = zeros(Float32,2,length(D))\n",
    "δ_history = zeros(Float32,length(D))\n",
    "R̂_history = zeros(Float32,length(D))\n",
    "preference_function_history = zeros(Float32,5,length(D))\n",
    "\n",
    "    \n",
    "# Basic tracker variables needed in order to simulate futures trading\n",
    "    \n",
    "# A round number to start our trading money at with as well as leverage typical of futures markets.\n",
    "# Training with realistic conditions such as leverage is important for the network to learn how exponential\n",
    "# changes in available capital between trades affects changes in rewards\n",
    "money = 1000.00f0\n",
    "leverage = 10.\n",
    "money_history = zeros(Float32,length(D))\n",
    "\n",
    "# Tracker arrays and variables for simulating the affects on our money of our trading decisions\n",
    "in_position_long = 0\n",
    "in_position_short = 0\n",
    "\n",
    "current_entrance = 0\n",
    "\n",
    "percentage_trade_history = zeros(length(D))\n",
    "percentage_trade = 1\n",
    "\n",
    "position_history = []\n",
    "action_history = zeros(Int64,length(D))\n",
    "\n",
    "position_change_history = zeros(Int64,length(D))\n",
    "\n",
    "return_history = zeros(length(D))\n",
    "\n",
    "transaction_cost = 0.9\n",
    "\n",
    "history = Float32[]\n",
    "    \n",
    "# Removing any current memory of the LSTMs\n",
    "Flux.reset!(Preference_network)\n",
    "Flux.reset!(m)\n",
    "Flux.reset!(State_value_function)\n",
    "\n",
    "R̂ = 0\n",
    "z_w = []\n",
    "for p in Flux.params(State_value_function)\n",
    "    if length(size(p)) == 1\n",
    "        push!(z_w,zeros(length(p)))\n",
    "    else\n",
    "        push!(z_w,zeros(size(p)[1],size(p)[2]))\n",
    "    end\n",
    "end\n",
    "\n",
    "z_θ = []\n",
    "for p in Flux.params(Preference_network)\n",
    "    if length(size(p)) == 1\n",
    "        push!(z_θ,zeros(length(p)))\n",
    "    else\n",
    "        push!(z_θ,zeros(size(p)[1],size(p)[2]))\n",
    "    end\n",
    "end\n",
    "    \n",
    "    \n",
    "# Each iteration of this loop is a training run over the whole dataset\n",
    "for c=1:1\n",
    "        \n",
    "    # Removing any current memory of the LSTMs\n",
    "    Flux.reset!(Preference_network)\n",
    "    Flux.reset!(m)\n",
    "    Flux.reset!(State_value_function)\n",
    "            \n",
    "\n",
    "    if TEST == false \n",
    "        θ_prev = deepcopy(Flux.params(Preference_network))\n",
    "        w_prev = deepcopy(Flux.params(State_value_function))\n",
    "    else\n",
    "        θ = Flux.params(Preference_network)\n",
    "        i = 0\n",
    "        for p in θ\n",
    "            i += 1\n",
    "            p .= θ_prev[i]\n",
    "        end\n",
    "\n",
    "        w = Flux.params(State_value_function)\n",
    "        i = 0\n",
    "        for p in w\n",
    "            i += 1\n",
    "            p .= w_prev[i]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Initialising the reward exponential moving average and the eligibility traces for the \n",
    "    # state value function and preference network\n",
    "    R̂ = 0\n",
    "    z_w = []\n",
    "    for p in Flux.params(State_value_function)\n",
    "        if length(size(p)) == 1\n",
    "            push!(z_w,zeros(length(p)))\n",
    "        else\n",
    "            push!(z_w,zeros(size(p)[1],size(p)[2]))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    z_θ = []\n",
    "    for p in Flux.params(Preference_network)\n",
    "        if length(size(p)) == 1\n",
    "            push!(z_θ,zeros(length(p)))\n",
    "        else\n",
    "            push!(z_θ,zeros(size(p)[1],size(p)[2]))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Arrays to store interesting quantities for seeing later\n",
    "    state_value_function_history = zeros(Float32,2,length(D))\n",
    "    δ_history = zeros(Float32,length(D))\n",
    "    R̂_history = zeros(Float32,length(D))\n",
    "    preference_function_history = zeros(Float32,5,length(D))\n",
    "\n",
    "\n",
    "    # Basic tracker variables needed in order to simulate futures trading\n",
    "\n",
    "    # A round number to start our trading money at with as well as leverage typical of futures markets\n",
    "    money = 1000.00f0\n",
    "    leverage = 10.\n",
    "    money_history = zeros(Float32,length(D))\n",
    "\n",
    "    # Tracker arrays and variables for simulating the affects on our money of our trading decisions\n",
    "    in_position_long = 0\n",
    "    in_position_short = 0\n",
    "\n",
    "    current_entrance = 0\n",
    "\n",
    "    percentage_trade_history = zeros(length(D))\n",
    "    percentage_trade = 1\n",
    "\n",
    "    position_history = []\n",
    "    action_history = zeros(Int64,length(D))\n",
    "\n",
    "    position_change_history = zeros(Int64,length(D))\n",
    "\n",
    "    return_history = zeros(length(D))\n",
    "\n",
    "    transaction_cost = 0.9\n",
    "\n",
    "    history = Float32[]\n",
    "        \n",
    "    # Get the m network to give its price predictions to be drawn on at each timestep\n",
    "    NN_res_list = m(reduce(hcat,seq[_begin_:_end_]))\n",
    "    \n",
    "    \n",
    "    last_NN_res = []\n",
    "        \n",
    "    # The loop over each timestep of the data\n",
    "    for t=1:length(D)\n",
    "        \n",
    "        # The price prediction given at the current time step t\n",
    "        NN_res = NN_res_list[:,t]\n",
    "        \n",
    "        \n",
    "        # Here we organise the current environment state and retrieve an action choice from the policy\n",
    "        if t === 1\n",
    "            \n",
    "            state = Float32[NN_res[1],NN_res[2],0.f0,0.f0,0.f0,0.f0,in_position_long,in_position_short,\n",
    "                    sde_data[1,t],sde_data[2,t],sde_data[3,t],sde_data[4,t],sde_data[5,t],\n",
    "                    sde_data[6,t],sde_data[7,t],sde_data[8,t],sde_data[9,t],sde_data[10,t],sde_data[11,t]]\n",
    "                        \n",
    "\n",
    "            action_choice, history, preference = Policy_choice(state)\n",
    "            \n",
    "            preference_function_history[1:3,t] = copy(preference)\n",
    "            preference_function_history[4:5,t] = copy(history)\n",
    "\n",
    "            last_NN_res = copy(NN_res)\n",
    "\n",
    "        else\n",
    "        \n",
    "            state = Float32[NN_res[1],NN_res[2],last_NN_res[1],last_NN_res[2],history[1],history[2],\n",
    "                    in_position_long,in_position_short,sde_data[1,t],sde_data[2,t],sde_data[3,t],sde_data[4,t],\n",
    "                    sde_data[5,t],sde_data[6,t],sde_data[7,t],sde_data[8,t],sde_data[9,t],sde_data[10,t],\n",
    "                    sde_data[11,t]]\n",
    "\n",
    "            \n",
    "            action_choice, history, preference = Policy_choice(state)\n",
    "            \n",
    "            preference_function_history[1:3,t] = copy(preference)\n",
    "            preference_function_history[4:5,t] = copy(history)\n",
    "            \n",
    "            last_NN_res = NN_res\n",
    "        end\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Here we record the changes done to our futures holding and money reserve as a result\n",
    "        # of the action choice for this timestep. The elseifs with ### beside them are the action\n",
    "        # of purchasing or selling, resulting in a change of position, the rest are simply data storage\n",
    "        # if the action choice is identical to our current position or to do nothing\n",
    "    \n",
    "        if action_choice === 1 && in_position_long === 1 && in_position_short === 0\n",
    "            push!(position_history,(in_position_long,in_position_short))\n",
    "        \n",
    "        # Buy if not in a position\n",
    "        elseif action_choice === 1 && in_position_long === 0 && in_position_short === 0     ###\n",
    "            current_entrance = D[t]\n",
    "            in_position_long = 1\n",
    "            position_change_history[t] = action_choice\n",
    "            push!(position_history,(in_position_long,in_position_short))\n",
    "        \n",
    "        # Buy if in a short position\n",
    "        elseif action_choice === 1 && in_position_long === 0 && in_position_short === 1     ###\n",
    "            change = (current_entrance-transaction_cost)/D[t]\n",
    "            return_history[t] = money*(leverage)*(change) - money*(leverage) #change - 1\n",
    "            money += return_history[t] #money*(leverage)*(change) - money*(leverage)\n",
    "\n",
    "            in_position_short = 0\n",
    "            in_position_long = 0\n",
    "            position_change_history[t] = action_choice\n",
    "            push!(position_history,(in_position_long,in_position_short))\n",
    "        \n",
    "        elseif action_choice === 2 && in_position_long === 0 && in_position_short === 1\n",
    "            push!(position_history,(in_position_long,in_position_short))\n",
    "        \n",
    "        # Sell if not in a position\n",
    "        elseif action_choice === 2 && in_position_long === 0 && in_position_short === 0     ###\n",
    "            current_entrance = D[t]\n",
    "            in_position_short = 1\n",
    "            position_change_history[t] = action_choice\n",
    "            push!(position_history,(in_position_long,in_position_short))\n",
    "        \n",
    "        # Sell if in a long position\n",
    "        elseif action_choice === 2 && in_position_long === 1 && in_position_short === 0     ###\n",
    "            change = D[t]/(current_entrance+transaction_cost)\n",
    "            return_history[t] = money*(leverage)*(change) - money*(leverage) #change - 1\n",
    "            money += return_history[t] #money*(leverage)*(change) - money*(leverage)\n",
    "\n",
    "            in_position_long = 0\n",
    "            in_position_short = 0\n",
    "            position_change_history[t] = action_choice\n",
    "            push!(position_history,(in_position_long,in_position_short))\n",
    "        \n",
    "        elseif action_choice === 3\n",
    "            push!(position_history,(in_position_long,in_position_short))\n",
    "        end\n",
    "        \n",
    "          \n",
    "        # Record information\n",
    "        money_history[t] = money\n",
    "        R = return_history[t]\n",
    "        \n",
    "        \n",
    "        # Organise the new environment state as a result of the actions taken\n",
    "        new_state = Float32[NN_res[1],NN_res[2],last_NN_res[1],last_NN_res[2],history[1],history[2],\n",
    "                    in_position_long,in_position_short,sde_data[1,t],sde_data[2,t],sde_data[3,t],sde_data[4,t],\n",
    "                    sde_data[5,t],sde_data[6,t],sde_data[7,t],sde_data[8,t],sde_data[9,t],sde_data[10,t],\n",
    "                    sde_data[11,t]]        \n",
    "\n",
    "        \n",
    "        if t%1000 === 0\n",
    "            println(money_history[t] - 1000,\"           \",t)\n",
    "        end\n",
    "        \n",
    "        \n",
    "        # After taking an action and observing the new environment state we commence the on-line learning\n",
    "        # which is the implementation of the actor-critic with eligibility traces (continuing) algorithm\n",
    "\n",
    "            \n",
    "        # Evaluate the state value function at the initial state at time t\n",
    "        S = State_value_function(state)[1]\n",
    "        # Evaluate the state value function after taking the action at time t (buy, sell, do nothing)\n",
    "        Sₙ = State_value_function(new_state)[1]\n",
    "\n",
    "        state_value_function_history[1,t] = S\n",
    "        state_value_function_history[2,t] = Sₙ - S\n",
    "\n",
    "        # δ is the differential return used to modify the length of gradient descent step sizes\n",
    "        # of the state value function network and the preference network based on how \n",
    "        # valuable the state change was due to our action and how much reward the action created for us\n",
    "        δ = R - R̂ + Sₙ - S\n",
    "\n",
    "        # R hat is the exponential moving average of reward used to compare how well our current actions\n",
    "        # are to previous ones\n",
    "        R̂ = R̂ + η_R̂*δ\n",
    "\n",
    "        δ_history[t] = δ\n",
    "        R̂_history[t] = R̂\n",
    "\n",
    "        # Here we take the gradient of the state value function network and modify the eligibility\n",
    "        # traces z_w for the SVF by weighting them by λ_w and incorporating the current gradient\n",
    "        w = Flux.params(State_value_function)\n",
    "        ∇w = Flux.gradient(()->State_value_function(state)[1],w)\n",
    "\n",
    "        i = 0\n",
    "        for g in ∇w\n",
    "            i += 1\n",
    "            if typeof(g) != Nothing\n",
    "                z_w[i] .= λ_w*z_w[i] + g\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # The same is done here for the preference network with the additional step of weighting\n",
    "        # the current gradient by the policy softmax probabilities (the logarithmic gradient)\n",
    "        # which are retrieved from the π_ function call\n",
    "        πₜ = π_(action_choice,state)\n",
    "\n",
    "        θ = Flux.params(Preference_network)\n",
    "        ∇θ = Flux.gradient(()->π_(action_choice,state),θ)\n",
    "\n",
    "        i = 0\n",
    "        for g in ∇θ\n",
    "            i += 1\n",
    "            if typeof(g) != Nothing\n",
    "                z_θ[i] .= λ_θ*z_θ[i] + g/πₜ\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # Here we perform the parameter updates to the state value function network and the\n",
    "        # preference function network via a gradient descent now that we have updated the \n",
    "        # eligibility traces for both networks\n",
    "\n",
    "        # It can be seen here how the eligibility traces act as a memory of previous gradients\n",
    "        # and how the stepsize is modified by the differential return δ in order to reduce\n",
    "        # movement in directions where the state is not valuable and the rewards were poor\n",
    "        i = 0\n",
    "        for p in w\n",
    "            i += 1\n",
    "            p .= p + η_w*δ*z_w[i]\n",
    "        end\n",
    "\n",
    "        i = 0\n",
    "        for p in θ\n",
    "            i += 1\n",
    "            p .= p + η_θ*δ*z_θ[i]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    Flux.reset!(Preference_network)\n",
    "    Flux.reset!(m)\n",
    "    Flux.reset!(State_value_function)\n",
    "        \n",
    "    println(\"Return: \",money_history[end] - 1000,\"          \",\"Money: \",money)\n",
    "    push!(value_history,money_history[end] - 1000)\n",
    "    println(c)\n",
    "        \n",
    "end\n",
    "println(_end_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Plot of the state value function and its successive differences through the timesteps of the dataset</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](state_value_function_history.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Plot of the preference network's output (not action preferences) over the timesteps of the dataset</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](preference_network_history.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Plot of the differential return through the timesteps of the dataset</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](differential_return_history.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Plot of the exponential moving average of rewards through the timesteps of the dataset</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](ema_of_reward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Plot of the price chart over time from the dataset and points indicating the switching of position into longs and short with green and red dots respectively</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "![image](price_security_changes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Plot of the money balance through the timesteps of the dataset</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](money_history.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Plot of the difference in money balance at the end of each training iteration</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](value_history.png)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
